"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 ğŸŒŒ COSMIC AI - Document Upload Routes
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
from fastapi import APIRouter, UploadFile, File, BackgroundTasks, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.responses import FileResponse
from config.settings import settings
from utils.file_handler import validate_file, save_upload_file
from services.document_processor import process_document
from services.vector_store import get_vector_store
from services.progress_service import get_progress_tracker
from utils.logger import setup_logger

logger = setup_logger()
progress_tracker = get_progress_tracker()

router = APIRouter(prefix="/api", tags=["documents"])


from services.graph_service import get_graph_service

@router.post("/upload")
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ“¤ Upload and process a document           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    logger.info("â•" * 60)
    logger.info("ğŸ“¤ NEW DOCUMENT UPLOAD")
    logger.info("â•" * 60)
    logger.info(f"   â””â”€ Filename: {file.filename}")
    logger.info(f"   â””â”€ Content-Type: {file.content_type}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step 0: Clear Previous Data (Fresh Start)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("ğŸ§¹ Clearing previous data (Clean Slate Mode)...")
    try:
        get_graph_service().clear_all()
        get_vector_store().clear_all()
        logger.info("âœ¨ Previous data cleared successfully.")
    except Exception as e:
        logger.error(f"âš ï¸ Error clearing previous data: {e}")
        # We continue anyway, as it might just be empty

    # Validate file
    validate_file(file)
    
    # Save file to disk
    file_path, file_id = await save_upload_file(file)
    
    # Process document in background
    logger.info("ğŸ”„ Starting background processing...")
    
    # Mark as processing immediately so status endpoint finds it
    vector_store = get_vector_store()
    vector_store.mark_as_processing(file_id)
    
    background_tasks.add_task(process_document, file_path, file_id)
    
    return {
        "status": "processing",
        "file_id": file_id,
        "filename": file.filename,
        "message": "ğŸš€ Document uploaded! Processing in background..."
    }


@router.get("/analyze/status/{file_id}")
async def get_analysis_status(file_id: str):
    """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ“Š Get document analysis status            â”‚
    â”‚  (Enhanced with real-time progress)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    # Try to get from progress tracker first (more detailed)
    progress = progress_tracker.get_progress(file_id)
    
    if progress:
        logger.info(f"ğŸ“Š Status check for: {file_id} - {progress['stage']}")
        return progress
    
    # Fallback to vector store status
    vector_store = get_vector_store()
    status = vector_store.get_document_status(file_id)
    
    logger.info(f"ğŸ“Š Status check for: {file_id} - {status.get('status', 'unknown')}")
    
    return {
        "file_id": file_id,
        "status": status.get("status", "processing"),
        "chunks_count": status.get("chunks_count", 0),
        "stage": status.get("status", "unknown"),
        "progress": 0
    }


@router.websocket("/ws/progress/{file_id}")
async def websocket_progress(websocket: WebSocket, file_id: str):
    """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ”Œ WebSocket for Real-time Progress        â”‚
    â”‚  Connect to receive live processing updates â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    await websocket.accept()
    
    try:
        # Register this connection for updates
        await progress_tracker.register_connection(file_id, websocket)
        
        # Send current progress immediately
        current_progress = progress_tracker.get_progress(file_id)
        if current_progress:
            await websocket.send_json(current_progress)
        
        logger.info(f"ğŸ”Œ WebSocket connected for file: {file_id}")
        
        # Keep connection alive and wait for disconnect
        while True:
            # Just receive any messages (ping/pong)
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        logger.info(f"ğŸ”Œ WebSocket disconnected for file: {file_id}")
    finally:
        # Unregister connection
        await progress_tracker.unregister_connection(file_id, websocket)


@router.get("/documents/{file_id}/view")
async def view_document(file_id: str):
    """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ‘ï¸ Serve document for PDF/Text preview     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    logger.info(f"ğŸ‘ï¸ Document view request: {file_id}")
    
    # Search for file in upload directory
    for filename in os.listdir(settings.UPLOAD_DIR):
        if filename.startswith(file_id):
            file_path = os.path.join(settings.UPLOAD_DIR, filename)
            logger.info(f"   â””â”€ Serving: {file_path}")
            return FileResponse(file_path)
    
    logger.warning(f"   â””â”€ File not found: {file_id}")
    raise HTTPException(status_code=404, detail="File not found")


@router.get("/documents/{file_id}/text-preview")
async def get_document_text(file_id: str):
    """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ“„ Get extracted text from vector store    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    logger.info(f"ğŸ“„ Text preview request: {file_id}")
    
    vector_store = get_vector_store()
    chunks = vector_store.get_all_chunks_for_file(file_id)
    
    if not chunks:
        logger.warning(f"   â””â”€ No chunks found for: {file_id}")
        raise HTTPException(status_code=404, detail="Document content not found")
    
    logger.info(f"   â””â”€ Returning {len(chunks)} chunks")
    
    return {
        "content": "\n\n".join(chunks),
        "chunks_count": len(chunks)
    }


@router.post("/clear-all")
async def clear_all_data():
    """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ—‘ï¸ CLEAR ALL DATA - Fresh Start             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Clears all databases and uploaded files.
    Use this when starting a new session or project.
    """
    
    logger.info("â•" * 60)
    logger.info("ğŸ—‘ï¸ CLEAR ALL DATA REQUEST")
    logger.info("â•" * 60)
    
    try:
        # Clear Vector Store (includes FAISS index, chunks, uploaded files, BM25)
        vector_store = get_vector_store()
        vector_store.clear_all()
        
        # Clear Graph Database
        from services.graph_service import get_graph_service
        graph_service = get_graph_service()
        graph_service.clear_all()
        
        logger.info("âœ… All data cleared successfully!")
        logger.info("â•" * 60)
        
        return {
            "status": "success",
            "message": "âœ… All data cleared! Ready for fresh start.",
            "details": {
                "vector_store": "cleared",
                "graph_database": "cleared",
                "bm25_index": "cleared",
                "uploaded_files": "cleared"
            }
        }
    
    except Exception as e:
        logger.error(f"âŒ Error clearing data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear data: {str(e)}")

