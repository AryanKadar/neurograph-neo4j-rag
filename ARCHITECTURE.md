# ğŸ—ï¸ Architecture Documentation

## Table of Contents
- [System Overview](#system-overview)
- [Modular Design](#modular-design)
- [Chunking Strategies](#chunking-strategies)
- [Vector Search Architecture](#vector-search-architecture)
- [RAG Pipeline](#rag-pipeline)
- [Data Flow](#data-flow)
- [API Design](#api-design)

---

## System Overview

This is an **Advanced Modular RAG (Retrieval-Augmented Generation)** system that combines modern AI techniques to deliver context-aware conversational responses.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Advanced Modular RAG Chatbot                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚                               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Frontend      â”‚           â”‚     Backend       â”‚
       â”‚   (React +      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   (FastAPI +      â”‚
       â”‚   TypeScript)   â”‚   REST    â”‚    Python)        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   API      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                          â”‚                 â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
            â”‚  Azure OpenAI   â”‚       â”‚  FAISS Vector   â”‚  â”‚  Document â”‚
            â”‚   GPT-5 API     â”‚       â”‚     Store       â”‚  â”‚  Processorâ”‚
            â”‚  (Chat + Embed) â”‚       â”‚   (HNSW Index)  â”‚  â”‚  (Chunker)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components
1. **Frontend**: React-based UI with real-time streaming
2. **Backend**: FastAPI async server with modular services
3. **LLM**: Azure OpenAI GPT-5 for chat and embeddings
4. **Vector DB**: FAISS with HNSW indexing
5. **Document Processing**: Multi-strategy chunking pipeline

---

## Modular Design

### Design Principles
The system follows the **Strategy Pattern** and **Dependency Injection** to enable:
- âœ… **Pluggability**: Swap components without changing core logic
- âœ… **Testability**: Mock services for unit testing
- âœ… **Scalability**: Add new strategies without breaking existing code
- âœ… **Maintainability**: Clear separation of concerns

### Service Architecture

```
Backend/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ chunking.py           # Text chunking strategies
â”‚   â”‚   â”œâ”€â”€ RecursiveTextChunker
â”‚   â”‚   â”œâ”€â”€ AgenticTextChunker
â”‚   â”‚   â””â”€â”€ get_text_chunker()  # Factory function
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings.py         # Embedding generation
â”‚   â”‚   â””â”€â”€ EmbeddingService
â”‚   â”‚
â”‚   â”œâ”€â”€ vector_store.py       # FAISS operations
â”‚   â”‚   â””â”€â”€ VectorStore
â”‚   â”‚
â”‚   â”œâ”€â”€ chat_service.py       # RAG chat logic
â”‚   â”‚   â””â”€â”€ ChatService
â”‚   â”‚
â”‚   â””â”€â”€ response_formatter.py # Response post-processing
â”‚       â””â”€â”€ ResponseFormatter
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ chat.py              # Chat endpoints
â”‚   â””â”€â”€ upload.py            # Document upload
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # Centralized configuration
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ logger.py            # Logging utilities
    â””â”€â”€ file_handler.py      # File operations
```

---

## Chunking Strategies

The system implements **two distinct chunking strategies** that can be switched via configuration:

### Strategy 1: Recursive Character Chunking

**When to use**: 
- Large document volumes
- Speed is critical
- Consistent, predictable chunking needed

**How it works**:
```
1. Start with full document
2. Try splitting on paragraph breaks (\n\n)
3. If chunks still too large, split on sentences (. )
4. If still too large, split on words ( )
5. Apply overlap between chunks
```

**Parameters**:
```python
CHUNK_SIZE = 1000        # Target tokens per chunk
CHUNK_OVERLAP = 200      # Overlap in tokens
MIN_CHUNK_SIZE = 100     # Discard smaller chunks
```

**Advantages**:
- âš¡ Fast processing
- ğŸ“ Consistent chunk sizes
- ğŸ¯ Preserves context with overlap
- ğŸ’° No API costs

**Code Flow**:
```python
RecursiveTextSplitter(
    separators=["\n\n", "\n", ". ", ", ", " ", ""],
    chunk_size=4000,    # chars (approx 1000 tokens)
    chunk_overlap=800   # chars (approx 200 tokens)
)
```

---

### Strategy 2: Agentic Chunking (LLM-Powered)

**When to use**:
- Quality over speed
- Complex, multi-topic documents
- Need semantic coherence

**How it works**:
```
1. Split text into atomic sentences
2. Format sentences in TOON (Token-Oriented Object Notation)
3. Pass batches to GPT-5 for topic boundary detection
4. LLM returns indices where new topics start
5. Merge sentences between breakpoints into chunks
```

**TOON Format Example**:
```
{index, content}
[5]
0   The Earth orbits the Sun.
1   This takes approximately 365 days.
2   Mars is the next planet out.
3   It has two moons named Phobos and Deimos.
4   The asteroid belt lies between Mars and Jupiter.
```

**Parameters**:
```python
AGENTIC_WINDOW_SIZE = 20  # Sentences per batch
```

**LLM Prompt**:
```
System: You are an expert Document Segmenter. 
        Identify logical breakpoints where a NEW topic 
        or distinct sub-topic begins.
        Output ONLY a JSON list of indices (e.g. [0, 5, 12]).

User: Analyze these sentences provided in TOON format:
      <TOON formatted text>
      Return valid start indices for new chunks.
```

**Advantages**:
- ğŸ§  Semantic awareness
- ğŸ¯ Topic coherence
- ğŸ“š Better for complex documents
- ğŸ” Improves retrieval quality

**Trade-offs**:
- ğŸ’° API costs (GPT calls per window)
- â±ï¸ Slower processing
- ğŸ”Œ Requires internet connection

**Code Flow**:
```python
1. _split_into_sentences(text)
   â””â”€> LangChain RecursiveCharacterTextSplitter
       with sentence separators

2. _find_breakpoints(sentences)
   â””â”€> Batch sentences into windows
       â””â”€> For each window:
           â”œâ”€> _format_sentences_to_toon()
           â”œâ”€> Call GPT-5 API
           â””â”€> Extract breakpoint indices

3. _merge_sentences(sentences, breakpoints)
   â””â”€> Combine sentences between breakpoints
   â””â”€> Filter by minimum size
```

---

## Vector Search Architecture

### FAISS with HNSW

**FAISS** (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors.

**HNSW** (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search.

### How HNSW Works

```
Layer 2 (Top):    A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ B
                  â”‚                   â”‚
                  â”‚                   â”‚
Layer 1:          A â”€â”€â”€â”€ C â”€â”€â”€â”€ D â”€â”€â”€â”€ B
                  â”‚      â”‚      â”‚      â”‚
                  â”‚      â”‚      â”‚      â”‚
Layer 0 (Base):   A â”€â”€ C â”€ E â”€ D â”€ F â”€ B â”€ G
```

**Search Process**:
1. Start at top layer
2. Navigate to closest neighbor
3. Drop down a layer
4. Repeat until reaching base layer
5. Return nearest k neighbors

### Configuration Parameters

```python
HNSW_M = 32
# Number of bi-directional links per node
# Higher M = Better recall, more memory
# Recommended: 16-64

HNSW_EF_CONSTRUCTION = 200
# Dynamic candidate list size during INDEX BUILD
# Higher = Better quality index, slower build
# Recommended: 100-500

HNSW_EF_SEARCH = 100
# Dynamic candidate list size during SEARCH
# Higher = Better accuracy, slower search
# Recommended: 50-200
```

### Performance Characteristics

| Vectors | M | EF_Search | QPS | Recall@10 |
|---------|---|-----------|-----|-----------|
| 10K     | 32| 100       | 5000| 95%       |
| 100K    | 32| 100       | 3000| 93%       |
| 1M      | 48| 150       | 1500| 95%       |

### Embedding Dimensions

**Azure OpenAI `text-embedding-ada-002`**:
- Dimension: 1536
- Use case: Production, best quality
- Cost: $0.0001 / 1K tokens

**Local `all-mpnet-base-v2`**:
- Dimension: 768
- Use case: Development, cost-sensitive
- Cost: Free (runs locally)

---

## RAG Pipeline

### Full Document Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Uploadsâ”‚
â”‚  Document   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document Parser     â”‚
â”‚ (PDF/DOCX/TXT/MD)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Raw Text
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Chunker       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Recursive   â”‚  â”‚
â”‚  â”‚     or       â”‚  â”‚â—„â”€â”€ CHUNKING_STRATEGY
â”‚  â”‚   Agentic    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Chunks []
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Service   â”‚
â”‚ (Azure OpenAI /     â”‚
â”‚  Sentence Trans.)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Embeddings []
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Store        â”‚
â”‚ (FAISS HNSW)        â”‚
â”‚ - Add vectors       â”‚
â”‚ - Save index        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Stored
       â–¼
   âœ… Ready for queries
```

### Query Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Service   â”‚
â”‚ (Embed query)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Query Vector
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Store        â”‚
â”‚ HNSW Search         â”‚
â”‚ - Find top-k        â”‚
â”‚ - Filter by score   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Retrieved Chunks
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context Builder     â”‚
â”‚ - Combine chunks    â”‚
â”‚ - Format context    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Context String
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Constructor  â”‚
â”‚ System + Context +  â”‚
â”‚ User Query          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Final Prompt
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure OpenAI GPT-5  â”‚
â”‚ (Streaming)         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Tokens
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Response Formatter  â”‚
â”‚ - Structure         â”‚
â”‚ - Markdown          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User UI   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prompt Template

```python
SYSTEM_PROMPT = """
You are a helpful AI assistant with access to document context. 
Answer questions based on the provided context when available.
If the context doesn't contain relevant information, say so.
Format responses professionally with markdown.
"""

CONTEXT_INJECTION = """
### Relevant Context:
{retrieved_chunks}

---

### User Question:
{user_query}
"""
```

---

## Data Flow

### Document Upload API Flow

```python
POST /api/upload
â”œâ”€> Validate file type
â”œâ”€> Save to uploads/
â”œâ”€> Parse document
â”‚   â”œâ”€> PDF: PyPDF2
â”‚   â”œâ”€> DOCX: python-docx
â”‚   â””â”€> TXT/MD: Direct read
â”œâ”€> Chunk text (strategy-dependent)
â”œâ”€> Generate embeddings (batch)
â”œâ”€> Add to FAISS index
â””â”€> Return success + chunk count
```

### Chat API Flow

```python
POST /api/chat/stream
â”œâ”€> Receive user message
â”œâ”€> Embed query
â”œâ”€> Search FAISS (top-k=5)
â”œâ”€> Build context from chunks
â”œâ”€> Construct prompt
â”œâ”€> Stream GPT-5 response
â”‚   â””â”€> Server-Sent Events (SSE)
â””â”€> Format and send tokens
```

---

## API Design

### REST Endpoints

#### Chat Endpoints

**1. Standard Chat**
```http
POST /api/chat
Content-Type: application/json

{
  "message": "What is retrieval-augmented generation?",
  "conversation_id": "uuid-v4",
  "use_rag": true
}

Response:
{
  "response": "Retrieval-Augmented Generation (RAG) is...",
  "conversation_id": "uuid-v4",
  "sources": ["chunk_id_1", "chunk_id_2"],
  "metadata": {
    "model": "gpt-5-chat",
    "tokens_used": 234,
    "retrieval_time_ms": 45
  }
}
```

**2. Streaming Chat**
```http
POST /api/chat/stream
Content-Type: application/json

{
  "message": "Explain FAISS HNSW",
  "conversation_id": "uuid-v4"
}

Response: (Server-Sent Events)
data: {"type": "token", "content": "FAISS"}
data: {"type": "token", "content": " is"}
data: {"type": "token", "content": " a"}
...
data: {"type": "done"}
```

#### Upload Endpoints

```http
POST /api/upload
Content-Type: multipart/form-data

file: document.pdf

Response:
{
  "filename": "document.pdf",
  "chunks_created": 42,
  "embedding_dimension": 1536,
  "chunking_strategy": "agentic",
  "processing_time_ms": 3421,
  "status": "success"
}
```

#### Health Check

```http
GET /api/health

Response:
{
  "status": "healthy",
  "version": "1.0.0",
  "vector_store": {
    "total_vectors": 1234,
    "dimension": 1536
  },
  "services": {
    "azure_openai": "connected",
    "vector_store": "loaded"
  }
}
```

---

## Technology Stack

### Backend
- **FastAPI**: Modern async web framework
- **Uvicorn**: ASGI server with WebSocket support
- **Pydantic**: Data validation and settings
- **Azure OpenAI SDK**: LLM and embeddings
- **FAISS**: Vector similarity search
- **LangChain**: Text splitting utilities
- **Sentence Transformers**: Local embeddings (optional)

### Frontend
- **React 18**: UI library with hooks
- **TypeScript**: Type-safe development
- **Vite**: Fast build tool with HMR
- **Axios**: HTTP client with streaming
- **CSS3**: Custom cosmic animations

### Infrastructure
- **Azure OpenAI**: Managed GPT-5 API
- **FAISS**: In-memory vector store with disk persistence
- **File System**: Local document and index storage

---

## Performance Optimization

### Async Operations
```python
# All I/O operations are async
async def process_document(file):
    text = await parse_document(file)
    chunks = await chunk_text(text)
    embeddings = await generate_embeddings(chunks)
    await store_vectors(embeddings)
```

### Batch Processing
```python
# Embed chunks in batches
BATCH_SIZE = 32
for i in range(0, len(chunks), BATCH_SIZE):
    batch = chunks[i:i+BATCH_SIZE]
    embeddings = await embed_batch(batch)
```

### Caching
- FAISS index loaded once at startup
- Chunker instance singleton
- Embedding model loaded once

---

## Security Considerations

1. **API Key Management**: Environment variables only
2. **CORS**: Configured allowed origins
3. **File Upload**: Type and size validation
4. **Input Sanitization**: Pydantic models
5. **Rate Limiting**: (Future enhancement)

---

## Deployment Architecture

### Development
```
Local Machine
â”œâ”€â”€ Backend: http://localhost:8000
â””â”€â”€ Frontend: http://localhost:3000
```

### Production (Example)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend (Vercel) â”‚
â”‚   https://app.com   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ HTTPS
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend (Azure App) â”‚
â”‚ https://api.app.com â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Azure OpenAI API   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Monitoring & Logging

### Structured Logging
```python
logger.info(f"âœ‚ï¸ Recursive chunking: {len(text)} chars")
logger.info(f"ğŸ§  Agentic chunking: Generated {len(chunks)} chunks")
logger.info(f"ğŸ“Š Vector search: {top_k} results in {latency}ms")
```

### Metrics to Track
- Document processing time
- Chunking strategy performance
- Vector search latency
- API token usage
- Error rates

---

**Last Updated**: 2026-01-01  
**Version**: 1.0  
**Author**: Aryan Kadar
